{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "qYjdzPUM9Y2u"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Frd-Iq_IoQ",
        "outputId": "d6927f39-f5f9-4091-85f8-489a416a8bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "NOZdUt3DAHhq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9"
      ],
      "metadata": {
        "id": "N1p7s8BWHscQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/data', train=True, download=True, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "wQUbcY3pARZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9fe091b-d905-4f18-a5cf-1a2030203478"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalNeuralNetwork(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(ConvolutionalNeuralNetwork, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        nn.Flatten(),\n",
        "\n",
        "        nn.Linear(16*5*5, 128),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(64, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.model(X)"
      ],
      "metadata": {
        "id": "jMhFsOzCDiu9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "convolutional = ConvolutionalNeuralNetwork(num_classes=NUM_CLASSES)\n",
        "convolutional = convolutional.to(device)"
      ],
      "metadata": {
        "id": "r8xWdmnsGgyy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, loss_function, epochs, learning_rate):\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  train_epoch_losses = []\n",
        "  train_epoch_accuracies = []\n",
        "\n",
        "  val_epoch_losses = []\n",
        "  val_epoch_accuracies = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    train_current_epoch_loss = 0.0\n",
        "    train_current_epoch_correct_predictions = 0\n",
        "    train_current_epoch_total_predictions = 0\n",
        "\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(train_dataloader):\n",
        "      X_batch = X_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(X_batch)\n",
        "      loss = loss_function(outputs, y_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_current_epoch_loss += loss.item()\n",
        "      train_current_epoch_correct_predictions += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "      train_current_epoch_total_predictions += y_batch.size(0)\n",
        "\n",
        "    train_epoch_losses.append(train_current_epoch_loss / len(train_dataloader))\n",
        "    train_epoch_accuracies.append(train_current_epoch_correct_predictions / train_current_epoch_total_predictions)\n",
        "\n",
        "    print(f'Training, Epoch {epoch + 1}/{epochs}, Loss: {train_epoch_losses[-1]}, Accuracy: {train_epoch_accuracies[-1]}')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    val_current_epoch_loss = 0.0\n",
        "    val_current_epoch_correct_predictions = 0\n",
        "    val_current_epoch_total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (X_batch, y_batch) in enumerate(val_dataloader):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        outputs = model(X_batch)\n",
        "        loss = loss_function(outputs, y_batch)\n",
        "\n",
        "        val_current_epoch_loss += loss.item()\n",
        "        val_current_epoch_correct_predictions += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "        val_current_epoch_total_predictions += y_batch.size(0)\n",
        "\n",
        "    val_epoch_losses.append(val_current_epoch_loss / len(val_dataloader))\n",
        "    val_epoch_accuracies.append(val_current_epoch_correct_predictions / val_current_epoch_total_predictions)\n",
        "\n",
        "    print(f'Validation, Epoch {epoch + 1}/{epochs}, Loss: {val_epoch_losses[-1]}, Accuracy: {val_epoch_accuracies[-1]}')\n",
        "\n",
        "  return train_epoch_losses, train_epoch_accuracies, val_epoch_losses, val_epoch_accuracies"
      ],
      "metadata": {
        "id": "NkN3H__xbOO9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_results(train_epoch_losses, train_epoch_accuracies, val_epoch_losses, val_epoch_accuracies):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_epoch_losses, label='Training Loss', color='blue')\n",
        "  plt.plot(val_epoch_losses, label='Validation Loss', color='red')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training and Validation Losses')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(train_epoch_accuracies, label='Training Accuracy', color='blue')\n",
        "  plt.plot(val_epoch_accuracies, label='Validation Accuracy', color='red')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Training and Validation Accuracies')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "SrIC2im-fLMt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch_losses, train_epoch_accuracies, val_epoch_losses, val_epoch_accuracies = train_model(convolutional, train_dataloader, val_dataloader, nn.CrossEntropyLoss(), NUM_EPOCHS, LEARNING_RATE)\n",
        "plot_train_results(train_epoch_losses, train_epoch_accuracies, val_epoch_losses, val_epoch_accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "7S-e45K4fvnS",
        "outputId": "7e16ca8f-eddf-472f-ac7b-f7c300c2fb27"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-46c6d721b09e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvolutional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_train_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-ea7d23ca9cea>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, loss_function, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_current_epoch_total_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_as_effectful_op_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallthrough_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myLinear(nn.Module):\n",
        "  def __init__(self, in_features, out_features, device=None):\n",
        "    super(myLinear, self).__init__()\n",
        "\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "\n",
        "    self.weight = nn.parameter.Parameter(torch.empty(out_features, in_features, device=device))\n",
        "    self.bias = nn.parameter.Parameter(torch.empty(out_features, device=device))\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    scale = 1.0 / np.sqrt(self.in_features)\n",
        "\n",
        "    nn.init.uniform_(self.weight, -scale, scale)\n",
        "    nn.init.uniform_(self.bias, -scale, scale)\n",
        "\n",
        "  def forward(self, X):\n",
        "    expanded_X = X.unsqueeze(dim=2)\n",
        "    y = torch.matmul(self.weight, expanded_X)\n",
        "    expanded_bias = self.bias.unsqueeze(dim=0).unsqueeze(dim=2)\n",
        "    y = y + expanded_bias\n",
        "    y = y.squeeze(dim=2)\n",
        "    return y"
      ],
      "metadata": {
        "id": "z7O5ZtQMhwKb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear = nn.Linear(in_features=8, out_features=16, device=device)\n",
        "my_linear = myLinear(in_features=8, out_features=16, device=device)\n",
        "\n",
        "random_input = torch.randn(8)\n",
        "random_input = random_input.unsqueeze(dim=0)\n",
        "random_input = random_input.to(device)\n",
        "\n",
        "print('Mean Absolute Difference before copy:', torch.mean(torch.abs(linear(random_input) - my_linear(random_input))).item())\n",
        "\n",
        "my_linear.load_state_dict(linear.state_dict())\n",
        "\n",
        "print('Mean Absolute Difference after copy:', torch.mean(torch.abs(linear(random_input) - my_linear(random_input))).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M43sxlS1i2ZI",
        "outputId": "46a9f922-eef8-427a-9886-9e7c8a42dcfa"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Difference before copy: 0.5196319222450256\n",
            "Mean Absolute Difference after copy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myConv2d(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1):\n",
        "    super(myConv2d, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.padding = padding\n",
        "    self.stride = stride\n",
        "\n",
        "    scale = 1.0 / np.sqrt(in_channels * kernel_size * kernel_size)\n",
        "\n",
        "    self.weight = nn.parameter.Parameter(torch.rand(out_channels, in_channels, kernel_size, kernel_size))\n",
        "    self.bias = nn.parameter.Parameter(torch.rand(out_channels))\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    scale = 1.0 / np.sqrt(self.in_channels * self.kernel_size * self.kernel_size)\n",
        "\n",
        "    nn.init.uniform_(self.weight, -scale, scale)\n",
        "    nn.init.uniform_(self.bias, -scale, scale)\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch_size = X.shape[0]\n",
        "    num_channels = X.shape[1]\n",
        "    image_height = X.shape[2]\n",
        "    image_width = X.shape[3]\n",
        "\n",
        "    y = torch.zeros(\n",
        "      batch_size,\n",
        "      self.out_channels,\n",
        "      (image_height + 2 * self.padding - self.kernel_size // 2 - self.kernel_size // 2) // self.stride,\n",
        "      (image_width + 2 * self.padding - self.kernel_size // 2 - self.kernel_size // 2) // self.stride\n",
        "    )\n",
        "\n",
        "    if self.padding > 0:\n",
        "      X = nn.functional.pad(X, (self.padding, self.padding, self.padding, self.padding), mode='constant', value=0)\n",
        "\n",
        "    for image_idx in range(batch_size):\n",
        "      for i in range(self.kernel_size // 2, image_height + 2 * self.padding - self.kernel_size // 2, self.stride):\n",
        "        for j in range(self.kernel_size // 2, image_width + 2 * self.padding - self.kernel_size // 2, self.stride):\n",
        "          y[image_idx, : , i - self.kernel_size // 2, j - self.kernel_size // 2] = torch.sum(\n",
        "              self.weight *\n",
        "              X[image_idx, : ,\n",
        "                i - self.kernel_size // 2 : i + self.kernel_size // 2 + 1,\n",
        "                j - self.kernel_size // 2 : j + self.kernel_size // 2 + 1], dim=(1, 2, 3)\n",
        "          ) + self.bias\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "w4yqvv6o2xO4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, padding=2, stride=1)\n",
        "my_conv2d = myConv2d(in_channels=3, out_channels=8, kernel_size=5, padding=2, stride=1)\n",
        "\n",
        "CHOSEN_IMAGES_SIZE = 16\n",
        "permutation = torch.randperm(len(train_dataset))[:CHOSEN_IMAGES_SIZE]\n",
        "chosen_images = Subset(train_dataset, permutation)\n",
        "\n",
        "sum_absolute_difference = 0.0\n",
        "for image, label in chosen_images:\n",
        "  sum_absolute_difference += torch.abs(conv2d(image.unsqueeze(dim=0)) - my_conv2d(image.unsqueeze(dim=0))).sum().item()\n",
        "print('Mean Absolute Difference before copy:', sum_absolute_difference / CHOSEN_IMAGES_SIZE)\n",
        "\n",
        "my_conv2d.load_state_dict(conv2d.state_dict())\n",
        "\n",
        "sum_absolute_difference = 0.0\n",
        "for image, label in chosen_images:\n",
        "  sum_absolute_difference += torch.abs(conv2d(image.unsqueeze(dim=0)) - my_conv2d(image.unsqueeze(dim=0))).sum().item()\n",
        "print('Mean Absolute Difference after copy:', sum_absolute_difference / CHOSEN_IMAGES_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NPQR_R-MJBz",
        "outputId": "8f27445a-d644-43f8-e140-95dcca0ab027"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Difference before copy: 1911.7588806152344\n",
            "Mean Absolute Difference after copy: 0.00024753916841291357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myConvolutionalNeuralNetwork(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(myConvolutionalNeuralNetwork, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        myConv2d(in_channels=3, out_channels=8, kernel_size=5),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        myConv2d(in_channels=8, out_channels=16, kernel_size=5),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        nn.Flatten(),\n",
        "\n",
        "        myLinear(16*5*5, 128),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        myLinear(128, 64),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        myLinear(64, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.model(X)"
      ],
      "metadata": {
        "id": "LZFAqkXvdCWw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_convolutional = myConvolutionalNeuralNetwork(num_classes=NUM_CLASSES)\n",
        "my_convolutional = my_convolutional.to(device)\n",
        "\n",
        "my_convolutional.load_state_dict(convolutional.state_dict())"
      ],
      "metadata": {
        "id": "XxDyk6B8ebMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89650c2f-ff0f-4f15-a223-99607dd9dd89"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy_and_time(model, chosen_images):\n",
        "  images = torch.stack([image[0] for image in chosen_images])\n",
        "  labels = torch.tensor([image[1] for image in chosen_images])\n",
        "\n",
        "  num_correct_predictions = 0\n",
        "  num_total_predictions = 0\n",
        "\n",
        "  if device == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    '''\n",
        "    for image, label in zip(images, labels):\n",
        "      image = image.unsqueeze(dim=0).to(device)\n",
        "      label = label.unsqueeze(dim=0).to(device)\n",
        "\n",
        "      outputs = model(image)\n",
        "\n",
        "      num_correct_predictions += (outputs.argmax(dim=1) == label).sum().item()\n",
        "      num_total_predictions += label.size(0)\n",
        "    '''\n",
        "\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "\n",
        "    num_correct_predictions += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "    num_total_predictions += labels.size(0)\n",
        "\n",
        "  if device == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "  end_time = time.time()\n",
        "\n",
        "  return num_correct_predictions / num_total_predictions, (end_time - start_time) / len(chosen_images)"
      ],
      "metadata": {
        "id": "yci6beYPvmU9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHOSEN_IMAGES_SIZE = 32\n",
        "permutation = torch.randperm(len(train_dataset))[:CHOSEN_IMAGES_SIZE]\n",
        "chosen_images = Subset(train_dataset, permutation)\n",
        "\n",
        "convolutional_accuracy, convolutional_time = compute_accuracy_and_time(convolutional, chosen_images)\n",
        "my_convolutional_accuracy, my_convolutional_time = compute_accuracy_and_time(my_convolutional, chosen_images)\n",
        "\n",
        "print('Convolutional Accuracy:', convolutional_accuracy)\n",
        "print('Convolutional Time:', convolutional_time)\n",
        "print('My Convolutional Accuracy:', my_convolutional_accuracy)\n",
        "print('My Convolutional Time:', my_convolutional_time)\n",
        "\n",
        "print('Time Ratio:', my_convolutional_time / convolutional_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq05r9A3yOWO",
        "outputId": "7df4bcb9-a14a-4c18-f67e-48ddee4207d9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolutional Accuracy: 0.25\n",
            "Convolutional Time: 0.00027588754892349243\n",
            "My Convolutional Accuracy: 0.25\n",
            "My Convolutional Time: 0.05421710014343262\n",
            "Time Ratio: 196.51883658753948\n"
          ]
        }
      ]
    }
  ]
}